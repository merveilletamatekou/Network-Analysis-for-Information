# Network-Analysis-for-Information


# Analyse et Classification de Corpus Documentaire

## Introduction
Ce projet utilise des méthodes d'apprentissage automatique pour organiser, rechercher et analyser un corpus de documents académiques. En se focalisant sur les titres des documents, nous avons construit un modèle capable de classer et de regrouper efficacement le contenu thématique.

## Objectif
L'objectif de ce projet est de créer un système qui simplifie la navigation dans un grand ensemble de documents textuels, en les classant de manière cohérente dans des catégories prédéfinies.

## Méthodologie
- **Réduction de dimension :** Utilisation de techniques comme PCA ou t-SNE pour réduire la dimensionnalité des données.
- **Clustering :** Application du clustering spectral et de Louvain pour identifier les regroupements naturels dans les données.
- **Classification :** Emploi de la régression logistique pour classifier les documents dans des domaines spécifiques.
- **Moteur de recherche :** Développement d'un moteur de recherche basé sur la similarité cosine pour trouver des documents pertinents par rapport à des requêtes utilisateur.

## Résultats
Les modèles ont montré une capacité prometteuse à regrouper et à classer les documents selon leurs caractéristiques sémantiques, avec une précision élevée dans la prédiction des catégories de domaine.

## Technologies Utilisées
- Python
- Scikit-learn
- Matplotlib
- Transformers (pour les embeddings BERT)

## Installation
1. Cloner le dépôt.
2. Installer les dépendances via `pip install -r requirements.txt`.
3. Exécuter les notebooks Jupyter pour visualiser les analyses.

## Usage
Les scripts sont structurés pour une exécution en notebook Jupyter, avec des annotations pour guider à travers les étapes d'analyse et de classification.

## Contributeurs
- [Akouvi Tamatekou- Lea Dahmani]



## Remerciements
Merci à Persee.fr pour le corpus de documents et à tous ceux qui ont contribué au projet.

---

Akouvi Tamatekou- Lea Dahmani
